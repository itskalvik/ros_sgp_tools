{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405685a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- NO TYPE 3 FONTS IN PDF OUTPUT ---\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams[\"pdf.fonttype\"] = 42\n",
    "mpl.rcParams[\"ps.fonttype\"] = 42\n",
    "mpl.rcParams[\"font.family\"] = \"DejaVu Sans\"\n",
    "mpl.rcParams[\"mathtext.fontset\"] = \"dejavusans\"\n",
    "mpl.rcParams[\"text.usetex\"] = False\n",
    "\n",
    "# Font sizes\n",
    "mpl.rcParams[\"font.size\"] = 14\n",
    "mpl.rcParams[\"axes.titlesize\"] = 14\n",
    "mpl.rcParams[\"axes.labelsize\"] = 14\n",
    "mpl.rcParams[\"xtick.labelsize\"] = 14\n",
    "mpl.rcParams[\"ytick.labelsize\"] = 14\n",
    "mpl.rcParams[\"legend.fontsize\"] = 12\n",
    "# -------------------------------------\n",
    "\n",
    "import imageio.v2 as imageio\n",
    "from tqdm import trange\n",
    "\n",
    "import os\n",
    "import h5py\n",
    "import yaml\n",
    "import pickle\n",
    "import shapely\n",
    "import numpy as np\n",
    "from utils import LatLonStandardScaler, StandardScaler\n",
    "\n",
    "from sgptools.kernels import *\n",
    "from sgptools.utils.gpflow import *\n",
    "from sgptools.utils.metrics import *\n",
    "from sgptools.utils.misc import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8885b760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont2disc(\n",
    "    Xu: np.ndarray,\n",
    "    candidates: np.ndarray,\n",
    "    candidate_labels: Optional[np.ndarray] = None\n",
    ") -> Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Maps continuous space locations (`Xu`) to the closest points in a discrete\n",
    "    set of candidate locations (`candidates`) using a Hungarian algorithm\n",
    "    (linear sum assignment) for optimal matching. This ensures each `Xu` point\n",
    "    is matched to a unique candidate.\n",
    "\n",
    "    Args:\n",
    "        Xu (np.ndarray): (m, d); Continuous space points (e.g., optimized sensor locations).\n",
    "                         `m` is the number of points, `d` is the dimensionality.\n",
    "        candidates (np.ndarray): (n, d); Discrete set of candidate locations.\n",
    "                                 `n` is the number of candidates, `d` is the dimensionality.\n",
    "        candidate_labels (Optional[np.ndarray]): (n, 1); Optional labels corresponding to\n",
    "                                                the discrete set of candidate locations.\n",
    "                                                If provided, the matched labels are also returned.\n",
    "                                                Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]:\n",
    "        - If `candidate_labels` is None:\n",
    "            np.ndarray: (m, d); Discrete space points' locations (`Xu_X`),\n",
    "                        where each point in `Xu` is mapped to its closest\n",
    "                        unique point in `candidates`.\n",
    "        - If `candidate_labels` is provided:\n",
    "            Tuple[np.ndarray, np.ndarray]: (`Xu_X`, `Xu_y`).\n",
    "            `Xu_X` (np.ndarray): (m, d); The matched discrete locations.\n",
    "            `Xu_y` (np.ndarray): (m, 1); The labels corresponding to `Xu_X`.\n",
    "\n",
    "    Usage:\n",
    "        ```python\n",
    "        import numpy as np\n",
    "        from sgptools.utils.misc import cont2disc\n",
    "\n",
    "        # Example continuous points\n",
    "        continuous_points = np.array([[0.1, 0.1], [0.9, 0.9], [0.5, 0.5]])\n",
    "        # Example discrete candidates\n",
    "        discrete_candidates = np.array([[0.0, 0.0], [1.0, 1.0], [0.4, 0.6]])\n",
    "        # Example candidate labels (optional)\n",
    "        discrete_labels = np.array([[10.0], [20.0], [15.0]])\n",
    "\n",
    "        # 1. Map without labels\n",
    "        mapped_points = cont2disc(continuous_points, discrete_candidates)\n",
    "\n",
    "        # 2. Map with labels\n",
    "        mapped_points_X, mapped_points_y = cont2disc(continuous_points, discrete_candidates, discrete_labels)\n",
    "        ```\n",
    "    \"\"\"\n",
    "    # Sanity check to handle empty inputs gracefully\n",
    "    if len(candidates) == 0 or len(Xu) == 0:\n",
    "        if candidate_labels is not None:\n",
    "            return np.empty((0, Xu.shape[1])), np.empty((0, 1))\n",
    "        else:\n",
    "            return np.empty((0, Xu.shape[1]))\n",
    "        \n",
    "    # Compute pairwise Euclidean distances between candidates and Xu\n",
    "    dists = pairwise_distances(candidates, Y=Xu, metric='euclidean')\n",
    "\n",
    "    # Use the Hungarian algorithm (linear_sum_assignment) to find the optimal\n",
    "    # assignment of rows (candidates) to columns (Xu points) that minimizes\n",
    "    # the total cost (distances). `row_ind` gives the indices of the rows\n",
    "    # (candidates) chosen, `col_ind` gives the corresponding indices of `Xu`.\n",
    "    row_ind, col_ind = linear_sum_assignment(dists)\n",
    "\n",
    "    # Select the candidate locations that were matched to Xu points\n",
    "    Xu_X = candidates[row_ind].copy()\n",
    "\n",
    "    if candidate_labels is not None:\n",
    "        # If labels are provided, select the corresponding labels as well\n",
    "        Xu_y = candidate_labels[row_ind].copy()\n",
    "        return Xu_X, Xu_y, row_ind\n",
    "    else:\n",
    "        return Xu_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faccf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_log = \"../launch/data/auv\"\n",
    "num_samples = 5000\n",
    "controller = \"aqua2\"\n",
    "\n",
    "# data file\n",
    "fname = os.path.join(mission_log,\n",
    "                     \"mission-log.hdf5\")\n",
    "\n",
    "# load data\n",
    "with h5py.File(fname, \"r\") as f:\n",
    "    print(f.keys())\n",
    "    fence_vertices = f[\"fence_vertices\"][:].astype(float)\n",
    "    X = f[\"X\"][:].astype(float)\n",
    "    y = f[\"y\"][:].astype(float)\n",
    "\n",
    "    X_candidates = f[\"X_objective\"][:].astype(float)\n",
    "\n",
    "    X_init = f[\"X_init\"][:].astype(float)\n",
    "    y_init = f[\"y_init\"][:].astype(float)\n",
    "\n",
    "    for key in f.keys():\n",
    "        if 'initial' in key:\n",
    "            initial_path = f[key][:].astype(float)\n",
    "        elif 'coverage' in key:\n",
    "            sol_path = f[key][:].astype(float)\n",
    "\n",
    "print(f'Mission Log: {mission_log}')\n",
    "print(f'Number of initial data samples: {X_init.shape[0]}')\n",
    "print(f'Number of solution data samples: {X.shape[0]}')\n",
    "print(f'Number of reconstruction samples: {num_samples}')\n",
    "\n",
    "if controller == 'mavros':\n",
    "    X_scaler = LatLonStandardScaler()\n",
    "    X_scaler.fit(X_candidates)\n",
    "elif controller == 'aqua2':\n",
    "    X_scaler = StandardScaler()\n",
    "    X_scaler.fit(X_candidates)\n",
    "\n",
    "    # Change variance/scale parameter to ensure all axis are scaled to the same value\n",
    "    ind = np.argmax(X_scaler.var_)\n",
    "    X_scaler.var_ = np.ones(X.shape[-1])*X_scaler.var_[ind]\n",
    "    X_scaler.scale_ = np.ones(X.shape[-1])*X_scaler.scale_[ind]\n",
    "    X_scaler.scale_ /= 10.0  # Scale to ensure an extent of ~10 units\n",
    "\n",
    "# Transform data\n",
    "X_candidates = X_scaler.transform(X_candidates)\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(y)\n",
    "\n",
    "y = y_scaler.transform(y)\n",
    "X = X_scaler.transform(X)\n",
    "\n",
    "initial_path = X_scaler.transform(initial_path)\n",
    "sol_path = X_scaler.transform(sol_path)[::-1]\n",
    "y_init = y_scaler.transform(y_init)\n",
    "X_init = X_scaler.transform(X_init)\n",
    "_, _, ind_init = cont2disc(initial_path, X_init, y_init)\n",
    "\n",
    "tmp1 = sol_path[-1].copy()\n",
    "tmp2 = sol_path[-2].copy()\n",
    "sol_path[-2] = tmp1\n",
    "sol_path[-1] = tmp2\n",
    "\n",
    "\n",
    "fence_vertices = X_scaler.transform(fence_vertices)\n",
    "sol_X, sol_y, ind = cont2disc(sol_path, X, y)\n",
    "\n",
    "# Load aqua data\n",
    "if controller == 'aqua2':\n",
    "    fname = os.path.join(mission_log, \"15-49.npy\")\n",
    "    data = np.load(fname)[280:2140]\n",
    "    X_data = data[:, :2]\n",
    "    y_data = data[:, 2].reshape(-1, 1)\n",
    "    X_data_all = X_scaler.transform(X_data)\n",
    "    y_data_all = y_scaler.transform(y_data)\n",
    "elif controller == 'mavros':\n",
    "    X_data_all = X\n",
    "    y_data_all = y\n",
    "X_data1, y_data1, ind_data1 = cont2disc(sol_path[:15], X_data_all[:255], y_data_all[:255])\n",
    "X_data2, y_data2, ind_data2 = cont2disc(sol_path[15:], X_data_all[255:], y_data_all[255:])\n",
    "X_data = np.vstack((X_data1, X_data2))\n",
    "y_data = np.vstack((y_data1, y_data2))\n",
    "ind_data = np.hstack((ind_data1, ind_data2+255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bccd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute grid\n",
    "distance = 0.3\n",
    "len_x = max(X_candidates[:, 0])-min(X_candidates[:, 0])\n",
    "len_y = max(X_candidates[:, 1])-min(X_candidates[:, 1])\n",
    "num_x = int(np.ceil(len_x/distance))\n",
    "num_y = int(np.ceil(len_y/distance))\n",
    "extent = [min(X_candidates[:, 0]), max(X_candidates[:, 0]), \n",
    "          min(X_candidates[:, 1]), max(X_candidates[:, 1])]\n",
    "grid_x, grid_y = np.mgrid[extent[0]:extent[1]:complex(num_x), \n",
    "                          extent[2]:extent[3]:complex(num_y)]\n",
    "X_grid = np.stack([grid_x, grid_y], axis=-1)\n",
    "X_grid = X_grid.reshape(-1, 2).astype(X_candidates.dtype)\n",
    "\n",
    "print(num_x, num_y)\n",
    "\n",
    "# Remove sensing locations outside the boundaries\n",
    "pbounds = shapely.geometry.Polygon(fence_vertices)\n",
    "points = shapely.points(X_grid)\n",
    "inside_idx = shapely.contains(pbounds, points)\n",
    "outside_idx = np.logical_not(inside_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201ac400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract hyperparameters details from config.yaml\n",
    "config_fname = os.path.join(mission_log, f\"config.yaml\")\n",
    "with open(config_fname, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "hyperparameter_config = config.get('hyperparameters', {})\n",
    "kernel = hyperparameter_config.get('kernel_function', 'RBF')\n",
    "\n",
    "# Use float32 and higher jitter for deep learning model based kernel functions\n",
    "if kernel in ['Attentive', 'NeuralSpectral']:\n",
    "    gpflow.config.set_default_float(np.float32)\n",
    "    gpflow.config.set_default_jitter(1e-1)\n",
    "else:\n",
    "    gpflow.config.set_default_float(np.float64)\n",
    "    gpflow.config.set_default_jitter(1e-6)\n",
    "\n",
    "kernel_kwargs = hyperparameter_config.get('kernel', {})\n",
    "kernel = get_kernel(kernel)(**kernel_kwargs)\n",
    "noise_variance = float(hyperparameter_config.get('noise_variance', 1e-4))\n",
    "optimizer_kwargs = config.get('optimizer', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f8a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Plot lengthscale map ----\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "lengthscales = kernel.get_lengthscales(X_grid.astype(np.float32))\n",
    "lengthscales[outside_idx] = np.nan\n",
    "\n",
    "im = ax.imshow(\n",
    "    lengthscales.reshape(num_x, num_y).T,\n",
    "    origin='lower',\n",
    "    extent=extent,\n",
    "    cmap='plasma'\n",
    ")\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.08)  # tweak size/pad as desired\n",
    "fig.colorbar(im, cax=cax)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('Lengthscale Predictions\\n')\n",
    "fig.savefig(\"lengthscales\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72f397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init GP model\n",
    "_, _, _, gpr = get_model_params(np.vstack([X_init]).astype(np.float32), \n",
    "                                np.vstack([y_init]).astype(np.float32), \n",
    "                                kernel=kernel,\n",
    "                                noise_variance=noise_variance,\n",
    "                                return_model=True,\n",
    "                                force_gp=True,\n",
    "                                verbose=False,\n",
    "                                max_steps=0)\n",
    "\n",
    "# Load pre-trained parameters\n",
    "fname = os.path.join(mission_log, f\"model_params.pkl\")\n",
    "with open(fname, 'rb') as handle:\n",
    "    params = pickle.load(handle)\n",
    "gpflow.utilities.multiple_assign(gpr.kernel, params['kernel'])\n",
    "gpflow.utilities.multiple_assign(gpr.likelihood, params['likelihood'])\n",
    "\n",
    "mean, var_prior = gpr.predict_f(X_candidates.astype(np.float32))\n",
    "print(\"Max Variance:\", var_prior.numpy().max())\n",
    "\n",
    "# ---- Plot variance map (prior) ----\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "_, var = gpr.predict_f(X_grid.astype(np.float32))\n",
    "var = var.numpy()\n",
    "var_min = var.min()\n",
    "var_max = var.max()\n",
    "\n",
    "var[outside_idx] = np.nan\n",
    "\n",
    "im = ax.imshow(\n",
    "    var.reshape(num_x, num_y).T,\n",
    "    origin='lower',\n",
    "    extent=extent,\n",
    "    vmin=var_min, \n",
    "    vmax=var_max,\n",
    "    cmap='plasma'\n",
    ")\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.08)  # tweak size/pad as desired\n",
    "fig.colorbar(im, cax=cax)\n",
    "\n",
    "if controller == 'mavros':\n",
    "    ax.plot(initial_path[:, 0], initial_path[:, 1], c='C2', marker='o', markersize=5, label='Planned Path')\n",
    "    ax.scatter(X_init[:, 0], X_init[:, 1], c='r', s=5, label='ASV Path/Collected Data', zorder=10, alpha=0.4)\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.legend(loc='lower left')\n",
    "ax.set_title(f'Prior Prediction Variance\\nMax Var: {var_prior.numpy().max():.2f}')\n",
    "fig.savefig(\"prior\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9fd700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init GP model\n",
    "_, _, _, gpr_post = get_model_params(np.vstack([X_data]).astype(np.float32), \n",
    "                                     np.vstack([y_data]).astype(np.float32), \n",
    "                                     kernel=kernel,\n",
    "                                     noise_variance=noise_variance,\n",
    "                                     return_model=True,\n",
    "                                     force_gp=True,\n",
    "                                     verbose=False,\n",
    "                                     max_steps=0)\n",
    "\n",
    "# Load pre-trained parameters\n",
    "fname = os.path.join(mission_log, f\"model_params.pkl\")\n",
    "with open(fname, 'rb') as handle:\n",
    "    params = pickle.load(handle)\n",
    "gpflow.utilities.multiple_assign(gpr_post.kernel, params['kernel'])\n",
    "gpflow.utilities.multiple_assign(gpr_post.likelihood, params['likelihood'])\n",
    "\n",
    "_, var_post = gpr_post.predict_f(X_candidates.astype(np.float32))\n",
    "print(\"Max Variance:\", var_post.numpy().max())\n",
    "\n",
    "# ---- Plot variance map (posterior) ----\n",
    "\n",
    "mean, var = gpr_post.predict_f(X_grid.astype(np.float32))\n",
    "mean = mean.numpy()\n",
    "var = var.numpy()\n",
    "var[outside_idx] = np.nan\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "im = ax.imshow(\n",
    "    var.reshape(num_x, num_y).T,\n",
    "    origin='lower',\n",
    "    extent=extent,\n",
    "    cmap='plasma'\n",
    ")\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"4%\", pad=0.08)  # thickness + spacing\n",
    "fig.colorbar(im, cax=cax)\n",
    "\n",
    "ax.plot(sol_path[:, 0], sol_path[:, 1], c='C2', marker='o', markersize=5, label='Planned Path')\n",
    "ax.plot(X_data_all[:, 0], X_data_all[:, 1], c='r', markersize=5, label='ASV Path', zorder=10, alpha=1)\n",
    "ax.scatter(X_data[:, 0], X_data[:, 1], c='r', s=25, label='Collected Data', zorder=10)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.legend(loc=\"lower left\",\n",
    "          bbox_to_anchor=(-0.2, -0.03),)\n",
    "ax.set_title(f'Posterior Prediction Variance\\nTarget Var: {var_prior.numpy().max()*0.5:.2f}; Max Var: {var_post.numpy().max():.2f}')\n",
    "fig.savefig(\"posterior\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "fps = 12\n",
    "out_mp4_prior = f\"{controller}_prior.mp4\"\n",
    "out_mp4_post  = f\"{controller}_posterior.mp4\"\n",
    "\n",
    "FIGSIZE = (7.2, 6.0)   # fixed canvas size\n",
    "DPI = 150              # fixed DPI\n",
    "\n",
    "X_grid_f32 = X_grid.astype(np.float32)\n",
    "X_candidates_f32 = X_candidates.astype(np.float32)\n",
    "\n",
    "TARGET_VAR = float(var_prior.numpy().max() * 0.5)\n",
    "\n",
    "# ----------------------------\n",
    "# GP + STATS HELPERS\n",
    "# ----------------------------\n",
    "def compute_variance_grid_flat(model):\n",
    "    \"\"\"Variance on grid as flat array with outside masked to NaN.\"\"\"\n",
    "    _, var = model.predict_f(X_grid_f32)\n",
    "    var = var.numpy().reshape(-1)\n",
    "    var[outside_idx] = np.nan\n",
    "    return var\n",
    "\n",
    "def compute_candidate_var_stats(model):\n",
    "    \"\"\"Mean/max variance on candidate set (for title text).\"\"\"\n",
    "    _, v = model.predict_f(X_candidates_f32)\n",
    "    v = v.numpy().reshape(-1)\n",
    "    return float(np.mean(v)), float(np.max(v))\n",
    "\n",
    "# ----------------------------\n",
    "# RENDERING\n",
    "# ----------------------------\n",
    "def render_frame(\n",
    "    var_flat,\n",
    "    planned_path,\n",
    "    asv_path_all,\n",
    "    visited_waypoints=None,     # planned waypoints to reveal progressively (e.g., initial_path)\n",
    "    visited_collected=None,     # collected data to reveal progressively (e.g., X_data)\n",
    "    k=0, t=0,\n",
    "    title_prefix=\"\",\n",
    "    var_mean=0.0,\n",
    "    var_max=0.0,\n",
    "    planned_label=\"Planned Path\",\n",
    "    visited_wp_label=\"Visited Waypoints\",\n",
    "    collected_label=\"Collected Data\",\n",
    "):\n",
    "    \"\"\"\n",
    "    One frame:\n",
    "      - variance heatmap\n",
    "      - planned path (remaining from last visited wp)\n",
    "      - ASV path (dense) + current ASV location\n",
    "      - optionally: visited waypoints (subset of visited_waypoints[:k])\n",
    "      - optionally: collected data (subset of visited_collected[:k])\n",
    "    \"\"\"\n",
    "    asv_current = asv_path_all[t]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=FIGSIZE, dpi=DPI)\n",
    "\n",
    "    im = ax.imshow(\n",
    "        var_flat.reshape(num_x, num_y).T,\n",
    "        origin=\"lower\",\n",
    "        extent=extent,\n",
    "        vmin=var_min,\n",
    "        vmax=var_max,\n",
    "        cmap=\"plasma\",\n",
    "    )\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"right\", size=\"4%\", pad=0.08)\n",
    "    fig.colorbar(im, cax=cax)\n",
    "\n",
    "    # Planned path: show remaining segment starting at the last visited waypoint (or start)\n",
    "    start_idx = max(k - 1, 0)\n",
    "    ax.plot(\n",
    "        planned_path[start_idx:, 0], planned_path[start_idx:, 1],\n",
    "        c=\"w\", marker=\"o\", markersize=5, linewidth=2,\n",
    "        label=planned_label, zorder=8\n",
    "    )\n",
    "\n",
    "    # Visited waypoints (subset of planned path)\n",
    "    if visited_waypoints is not None and k > 0:\n",
    "        ax.scatter(\n",
    "            visited_waypoints[:k, 0], visited_waypoints[:k, 1],\n",
    "            c=\"r\", s=30, zorder=10,\n",
    "            label=visited_wp_label\n",
    "        )\n",
    "\n",
    "    # Collected data points (subset of some data set, e.g., X_data)\n",
    "    if visited_collected is not None and k > 0:\n",
    "        ax.scatter(\n",
    "            visited_collected[:k, 0], visited_collected[:k, 1],\n",
    "            c=\"r\", s=30, zorder=10,\n",
    "            label=collected_label\n",
    "        )\n",
    "\n",
    "    # Current ASV location marker\n",
    "    ax.scatter(\n",
    "        asv_current[0], asv_current[1],\n",
    "        s=120, c=\"white\", edgecolor=\"black\", linewidth=1.2,\n",
    "        zorder=9, label=\"ASV Location\"\n",
    "    )\n",
    "\n",
    "    # Clean look / green-screen background (your original aesthetic)\n",
    "    ax.set_xlabel(None); ax.set_ylabel(None)\n",
    "    ax.set_xticks([]); ax.set_yticks([])\n",
    "    ax.set_frame_on(False)\n",
    "\n",
    "    ax.set_title(\n",
    "        f\"{title_prefix}\\n\"\n",
    "        f\"Target Var: {TARGET_VAR:.2f}; Max Var: {var_max:.2f}; Mean Var: {var_mean:.2f}\"\n",
    "    )\n",
    "\n",
    "    # De-duplicate legend labels\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    seen = set()\n",
    "    uniq_h, uniq_l = [], []\n",
    "    for h, lab in zip(handles, labels):\n",
    "        if lab not in seen:\n",
    "            uniq_h.append(h); uniq_l.append(lab); seen.add(lab)\n",
    "\n",
    "    ax.legend(\n",
    "        uniq_h, uniq_l,\n",
    "        loc=\"lower left\",\n",
    "        bbox_to_anchor=(0.45, 0.78),\n",
    "        facecolor=\"None\",\n",
    "        edgecolor=\"None\"\n",
    "    )\n",
    "\n",
    "    fig.set_facecolor(\"#39FF14\")\n",
    "    ax.set_facecolor(\"#39FF14\")\n",
    "    fig.patch.set_alpha(1.0)\n",
    "    ax.patch.set_alpha(1.0)\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    rgba = np.asarray(fig.canvas.buffer_rgba())\n",
    "    frame = rgba[..., :3].copy()\n",
    "    plt.close(fig)\n",
    "    return frame\n",
    "\n",
    "def update_gp_to_k_waypoints(k: int):\n",
    "    \"\"\"Update posterior GP training data using waypoint/data up to k (k in [0..n_wp]).\"\"\"\n",
    "    X_train = np.vstack([X_data[:k]]).astype(np.float32)\n",
    "    y_train = np.vstack([y_data[:k]]).astype(np.float32)\n",
    "\n",
    "    gpr_post.data = (X_train, y_train)\n",
    "\n",
    "# 2) POSTERIOR VIDEO\n",
    "#   planned path = sol_path\n",
    "#   ASV path      = X_data_all\n",
    "#   visited pts   = X_data[:k] where k increases when ind_data reached\n",
    "#   GP updates when a new waypoint is reached\n",
    "# ============================================================\n",
    "planned_path_post = np.asarray(sol_path)\n",
    "asv_path_all_post = np.asarray(X_data_all)\n",
    "T_post = len(asv_path_all_post)\n",
    "\n",
    "# ind_data: indices in X_data_all when each solution waypoint is hit\n",
    "ind_end = np.asarray(ind_data, dtype=int)\n",
    "ind_end = np.clip(ind_end, 0, T_post - 1)\n",
    "n_wp = len(X_data)\n",
    "\n",
    "last_k = 0\n",
    "update_gp_to_k_waypoints(last_k)\n",
    "#var_flat_post = compute_variance_grid_flat(gpr_post)\n",
    "#post_mean, post_max = compute_candidate_var_stats(gpr_post)\n",
    "\n",
    "with imageio.get_writer(out_mp4_post, fps=fps, codec=\"libx264\", quality=8, format=\"FFMPEG\") as writer:\n",
    "    for t in trange(15, T_post, desc=\"Rendering+encoding POSTERIOR (dense ASV timesteps)\"):\n",
    "        k = int(np.searchsorted(ind_end, t, side=\"right\"))  # 0..n_wp\n",
    "\n",
    "        if k != last_k:\n",
    "            update_gp_to_k_waypoints(k)\n",
    "            var_flat_post = compute_variance_grid_flat(gpr_post)\n",
    "            post_mean, post_max = compute_candidate_var_stats(gpr_post)\n",
    "            last_k = k\n",
    "\n",
    "        frame = render_frame(\n",
    "            var_flat=var_flat_post,\n",
    "            planned_path=planned_path_post,\n",
    "            asv_path_all=asv_path_all_post,\n",
    "            visited_waypoints=None,\n",
    "            visited_collected=X_data,   # reveal X_data[:k]\n",
    "            k=k, t=t,\n",
    "            title_prefix=\"Prediction Variance\",\n",
    "            var_mean=post_mean,\n",
    "            var_max=post_max,\n",
    "            planned_label=\"Planned Path\",\n",
    "            collected_label=\"Visited Waypoints\"\n",
    "        )\n",
    "        writer.append_data(frame)\n",
    "\n",
    "print(f\"Saved video: {out_mp4_post}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
